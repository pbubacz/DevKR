{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "FILE_NAME = Path(\"data/html/sample-html.html\")\n",
    "if not FILE_NAME.exists():\n",
    "    raise FileNotFoundError(f\"Expected to find {FILE_NAME} before running the notebook\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unstructured.io \n",
    "\n",
    "[Unstructured](https://github.com/Unstructured-IO/unstructured) is an open-source ETL toolkit that turns messy business documents—PDFs, Word files, HTML, even slide decks—into structured elements that downstream apps can understand. The library is modular, so you can:\n",
    "\n",
    "- Detect layout, clean markup, and normalize text with a single pipeline.\n",
    "- Partition documents into typed elements (titles, narrative text, tables, images, etc.).\n",
    "- Chunk content intelligently using document context such as headings or tables.\n",
    "\n",
    "In this notebook we focus on **chunking an HTML document**. The workflow looks like this:\n",
    "\n",
    "1. Partition the document into semantic elements.\n",
    "2. Chunk the elements by title so related bullet points stay together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet \"unstructured[html]\"  # Skip if the dependency is already satisfied\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import shorten\n",
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "elements = partition_html(filename=str(FILE_NAME))\n",
    "print(f\"Loaded {len(elements)} elements from {FILE_NAME.name}\")\n",
    "\n",
    "\n",
    "def _element_text(element):\n",
    "    if element.text:\n",
    "        return element.text\n",
    "    if getattr(element.metadata, \"text_as_html\", None):\n",
    "        return element.metadata.text_as_html\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "for idx, element in enumerate(elements):\n",
    "    element_text = _element_text(element)\n",
    "    preview = shorten(element_text.replace(\"\\n\", \" \").strip(), width=110, placeholder=\"...\")\n",
    "    print(f\"[{idx:>2}] {element.category:<12} ({len(element_text)} chars) {preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sections = defaultdict(list)\n",
    "current_title = \"Untitled Section\"\n",
    "\n",
    "for element in elements:\n",
    "    element_text = _element_text(element).strip()\n",
    "    if not element_text:\n",
    "        continue\n",
    "    if element.category == \"Title\":\n",
    "        current_title = element.text.strip() or current_title\n",
    "        continue\n",
    "    sections[current_title].append((element.category, element_text))\n",
    "\n",
    "print(f\"Detected {len(sections)} content sections with narrative text:\")\n",
    "for title, content in sections.items():\n",
    "    section_preview = shorten(\" \".join(segment for _, segment in content).replace(\"\\n\", \" \"), width=120, placeholder=\"...\")\n",
    "    print(f\"- {title}: {section_preview}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CHARACTERS = 1200\n",
    "NEW_AFTER_N_CHARS = 1600\n",
    "COMBINE_UNDER_N_CHARS = 150\n",
    "\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "chunks = chunk_by_title(\n",
    "    elements,\n",
    "    multipage_sections=True,\n",
    "    max_characters=MAX_CHARACTERS,\n",
    "    new_after_n_chars=NEW_AFTER_N_CHARS,\n",
    "    combine_text_under_n_chars=COMBINE_UNDER_N_CHARS,\n",
    ")\n",
    "\n",
    "chunk_records = []\n",
    "sum_chars = 0\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    raw_text = chunk.text or getattr(chunk.metadata, \"text_as_html\", \"\") or \"\"\n",
    "    preview = shorten(raw_text.replace(\"\\n\", \" \"), width=140, placeholder=\"...\")\n",
    "    print(f\"Chunk {idx:02d} [{chunk.category} | {len(raw_text)} chars]: {preview}\")\n",
    "    sum_chars += len(raw_text)\n",
    "\n",
    "    chunk_records.append(\n",
    "        {\n",
    "            \"idx\": idx,\n",
    "            \"category\": chunk.category,\n",
    "            \"chars\": len(raw_text),\n",
    "            \"preview\": preview,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"\\nCreated {len(chunk_records)} chunks (max {MAX_CHARACTERS} chars each).\")\n",
    "print(f\"Average characters per chunk: {sum_chars / len(chunk_records):.2f}\")\n",
    "print(f\"Total characters across all chunks: {sum_chars}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
