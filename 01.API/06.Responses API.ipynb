{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d3dc33",
   "metadata": {},
   "source": [
    "# The Responses API\n",
    "The Responses API is a new stateful API from Azure OpenAI. It brings together the best capabilities from the chat completions and assistants API in one unified experience. The Responses API also adds support for the new computer-use-preview model which powers the Computer use capability.\n",
    "## Region availability\n",
    "The responses API is currently available in the following regions:\n",
    "- australiaeast\n",
    "- eastus\n",
    "- eastus2\n",
    "- francecentral\n",
    "- japaneast\n",
    "- norwayeast\n",
    "- polandcentral\n",
    "- southindia\n",
    "- swedencentral\n",
    "- switzerlandnorth\n",
    "- uaenorth\n",
    "- uksouth\n",
    "- westus\n",
    "- westus3\n",
    "## Model support:\n",
    "- gpt-5-pro (Version: 2025-10-06)\n",
    "- gpt-5-codex (Version: 2025-09-11)\n",
    "- gpt-5 (Version: 2025-08-07)\n",
    "- gpt-5-mini (Version: 2025-08-07)\n",
    "- gpt-5-nano (Version: 2025-08-07)\n",
    "- gpt-5-chat (Version: 2025-08-07)\n",
    "- gpt-5-chat (Version: 2025-10-03)\n",
    "- gpt-5-codex (Version: 2025-09-15)\n",
    "- gpt-4o (Versions: 2024-11-20, 2024-08-06, 2024-05-13)\n",
    "- gpt-4o-mini (Version: 2024-07-18)\n",
    "- computer-use-preview\n",
    "- gpt-4.1 (Version: 2025-04-14)\n",
    "- gpt-4.1-nano (Version: 2025-04-14)\n",
    "- gpt-4.1-mini (Version: 2025-04-14)\n",
    "- gpt-image-1 (Version: 2025-04-15)\n",
    "- gpt-image-1-mini (Version: 2025-10-06)\n",
    "- o1 (Version: 2024-12-17)\n",
    "- o3-mini (Version: 2025-01-31)\n",
    "- o3 (Version: 2025-04-16)\n",
    "- o4-mini (Version: 2025-04-16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839739e9",
   "metadata": {},
   "source": [
    " # Note\n",
    "\n",
    "Not currently supported:\n",
    "\n",
    "- The web search tool\n",
    "- Image generation using multi-turn editing and streaming - coming soon\n",
    "- Images can't be uploaded as a file and then referenced as input. Coming soon.\n",
    "\n",
    "# Known issue \n",
    "There's a known issue with the following:\n",
    "- PDF as an input file is now supported, but setting file upload purpose to user_data is not currently supported.\n",
    "- Performance issues when background mode is used with streaming. The issue is expected to be resolved soon.\n",
    "\n",
    "## API reference\n",
    "- [Responses API reference](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview-latest#create-response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging to output to console\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "if load_dotenv():\n",
    "    url = f\"{os.getenv('AZURE_OPENAI_ENDPOINT')}/openai/v1\"\n",
    "    key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    model = os.getenv(\"AZURE_OPENAI_COMPLETION_MODEL\")\n",
    "\n",
    "    logger.info(f\"Using endpoint: {url}\")\n",
    "    logger.info(f\"Using key: {key[:4]}...{key[-4:]}\")\n",
    "    logger.info(f\"Using model: {model}\")\n",
    "\n",
    "else:\n",
    "    logger.error(\"No file .env found\")\n",
    "    exit(1)\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=key,\n",
    "    base_url=url,\n",
    ")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=model,  # Replace with your model deployment name\n",
    "    input=\"This is a test.\",\n",
    ")\n",
    "\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada60a85",
   "metadata": {},
   "source": [
    "# Retrieve a response\n",
    "To retrieve a response from a previous call to the responses API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198e3eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id=response.id\n",
    "response_retrived = client.responses.retrieve(response_id)\n",
    "print(response_retrived.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304aab2a",
   "metadata": {},
   "source": [
    "# Chaining responses together\n",
    "You can chain responses together by passing the response.id from the previous response to the previous_response_id parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=model, \n",
    "    input=\"Define and shortly explain the concept of catastrophic forgetting?\"\n",
    ")\n",
    "\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=model,\n",
    "    previous_response_id=response.id,\n",
    "    input=\"Explain this at a level that could be understood by a college freshman\"\n",
    ")\n",
    "\n",
    "print(\"First response:\")\n",
    "print(response.output_text)\n",
    "\n",
    "print(\"Second response:\")\n",
    "print(second_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f461ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_id=second_response.id\n",
    "response_retrived = client.responses.retrieve(response_id)\n",
    "print(response_retrived.model_dump_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
