# MCP Lab Exercise

This exercise guides you through exploring the provided Model Context Protocol (MCP) assets, extending the server, and wiring the client to new tooling. Complete each part in sequence and record observations along the way.

## Prerequisites

- Python 3.10+
- Node.js 18+
- Access to an Azure OpenAI resource with the environment variables referenced in `client/app.py` configured (`AZURE_OPENAI_COMPLETION_MODEL`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `OPENAI_API_VERSION`).
- Recommended: create and activate a fresh virtual environment before installing Python dependencies.

## Part 1: Inspect the Filesystem MCP Server

1. Open a terminal in `08.MCP`.
2. Launch the inspector with the filesystem server baked into `npx`:

   ```bash
   npx @modelcontextprotocol/inspector npx -y @modelcontextprotocol/server-filesystem .
   ```

3. Browse to <http://127.0.0.1:6274>, click **Connect**, and review the exposed tools and resources.
4. Capture at least two example tool invocations and note the schema fields each tool expects.
5. **Stop the inspector with `Ctrl+C` before moving on.**

## Part 2: Run the Chainlit Client Against the Filesystem Server

1. From `08.MCP/client`, install the Python dependencies:

   ```bash
   pip install -r requirements.txt
   ```

2. Start Chainlit:

   ```bash
   chainlit run app.py
   ```

3. In the Chainlit UI, click the plugin icon and connect a new MCP endpoint with:
   - Type: `stdio`
   - Name: `local`
   - Command: `npx -y @modelcontextprotocol/server-filesystem .`
4. Send natural language queries that trigger the filesystem tools (for example, "List the files in this directory" or "Show the contents of README.md").
5. Observe how streamed responses appear in the chat and identify where `client/app.py` maps MCP responses into Chainlit messages.

## Part 3: Explore the Provided FastMCP Server

1. Open `server/server.py` and note the two artifacts already registered:
   - The `add` tool
   - The `greeting://{name}` resource
2. Install server dependencies from `08.MCP/server`:

   ```bash
   pip install fastmcp
   ```

3. Start the development server:

   ```bash
   fastmcp dev server.py
   ```

4. Connect the Chainlit client to this server and call both the tool and the resource. Record the raw payloads returned by MCP.

## Part 4: Extend the Server

1. Add a new tool to `server/server.py` that surfaces a capability available from standard Python libraries (for example, looking up environment variables or parsing JSON). Include type hints and docstrings so the client can infer argument schemas.
2. Add a second `@mcp.resource` that returns structured data (JSON serialised string) derived from a text file located under `client/` or `server/`.
3. Restart the FastMCP server and confirm the Chainlit client lists both new capabilities. Capture screenshots or console logs as evidence.

## Part 5: Enhance the Client Experience

1. Inspect `client/app.py` and locate the `ChatClient.process_response_stream` method. Explain in your notes how the implementation handles streamed tool calls without recursion.
2. Modify the client (in a separate branch or local copy) to display a summary banner after each tool invocation. Suggested approach:
   - Hook into the `call_tool` step and append a short summary message to the chat transcript.
   - Ensure the banner includes the tool name, arguments used, and the number of returned items.
3. Test your enhancement with at least two tools (one from the filesystem server and one from your extended FastMCP server).

## Part 6: Optional Enhancements

- Update the FastMCP server to read configuration from environment variables instead of hardcoding metadata.
- Add automated tests for the new server functions using `pytest`.
- Integrate tracing via Chainlit callbacks to capture request/response telemetry.

## Deliverables

- Updated `server/server.py` and any client changes created during Parts 4 and 5 (keep original copies if required by your instructor).
- A short write-up summarising:
  - Tools and resources discovered in Part 1.
  - Screenshots or logs proving the client executed your new tool/resource.
  - Lessons learned while handling streamed tool responses in Chainlit.
- Optional: submit your enhancements as a pull request or share diffs for review.

Complete the exercise by reflecting on how MCP enables tool-based workflows and how Chainlit can orchestrate multi-step interactions with both filesystem and custom FastMCP servers.
